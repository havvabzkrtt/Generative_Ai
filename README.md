# Generative Ai

**Generative AI** => Derin Ã¶ÄŸrenmenin bir alt alanÄ±dÄ±r. Metin, resim, ses, video gibi yeni iÃ§erikler Ã¼retmek iÃ§in kullanÄ±lÄ±r.

**Dil Modeli:** SÃ¶zcÃ¼k dizileri Ã¼zerindeki bir olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±dÄ±r.

## ðŸ“Œ Large Language Model (LLM) Nedir?

LLM (BÃ¼yÃ¼k Dil Modeli), **insan dilini anlama ve Ã¼retme** yeteneÄŸine sahip, **Ã§ok bÃ¼yÃ¼k miktarda veriyle** eÄŸitilmiÅŸ **derin Ã¶ÄŸrenme** tabanlÄ± bir yapay zeka modelidir. Genellikle **metin oluÅŸturma, soru-cevap, Ã§eviri, duygu analizi** gibi birÃ§ok **doÄŸal dil iÅŸleme (NLP) gÃ¶revini** yerine getirebilir.

---

## ðŸ§  LLM Temel YapÄ±sÄ±

LLM'ler genellikle aÅŸaÄŸÄ±daki katmanlardan oluÅŸur:

* **Embedding Layer:** Kelimeleri sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
* **Feedforward Layer (FFN):** Temsil edilen kelimeleri iÅŸleyerek anlam Ã§Ä±karÄ±r.
* **Attention Mechanism:** CÃ¼mledeki Ã¶nemli kelimelere odaklanmayÄ± saÄŸlar.
* **Transformer Mimarisi:** Encoder ve Decoder yapÄ±larÄ± ile anlamlÄ± iÃ§erik Ã¼retimini mÃ¼mkÃ¼n kÄ±lar.

### ðŸ”„ Encoder-Decoder YapÄ±sÄ±

* **Encoder:** Girdiyi (prompt) analiz eder, anlamlandÄ±rÄ±r.
* **Decoder:** Girdiye dayalÄ± yeni iÃ§erik Ã¼retir (metin, cevap vb.).

---

## ðŸ§° Ã–nemli Teknolojiler ve Kavramlar

### ðŸŸ¢ Embedding Teknikleri

**Embedding:** Bir kelimenin veya metnin bir sayÄ±sal vektÃ¶rle temsil edilmesi iÅŸlemidir.


1) **Word2Vec (CBOW, Skip-gram)**

- **CBOW:** EtrafÄ±ndaki kelimelerden ortadaki kelimeyi bulmaya Ã§alÄ±ÅŸÄ±yor.
- **SKIPGRAM:** Ortadaki kelimelerden etrafÄ±ndaki kelimeleri bulmaya Ã§alÄ±ÅŸÄ±yor.

![alt text](images/word2vec1.png)

SayÄ±sal deÄŸerler daha hÄ±zlÄ± anlayabilmek iÃ§in renklendirilmiÅŸtir.

![alt text](images/word2vec2.png)

Kelimeler arasÄ±ndaki iliÅŸkilerin yakalanmasÄ±.

![alt text](images/word2vec3.png)

Kelimler vektÃ¶rleri Ã¼zerinde lineer cebir yapÄ±labilir.

2) **ELMo(Embedding from Language Model) (2018)** 

- BaÄŸlamsal embedding
- BaÄŸlam iÃ§inde kullanÄ±lÄ±r. Test sÄ±rasÄ±nda â€œappleâ€ Ä±n elma mÄ± yoksa marka mÄ± olduÄŸunu baÄŸlama gÃ¶re anlar. Word2vec bunu gerÃ§ekleÅŸtiremez, her iki â€œappleâ€ Ä±n vektÃ¶rÃ¼ aynÄ±dÄ±r.

3) **BERT:** 
- Ã‡ift yÃ¶nlÃ¼ baÄŸlamlÄ± embedding

### ðŸŸ£ Transformer Mimarisi (2017)

- "Attention Is All You Need" makalesiyle tanÄ±tÄ±ldÄ±. Modern LLM'lerin temelidir.

![alt text](images/transformer.png)

LLMâ€™nin en yaygÄ±n mimarisidir. Transformer modeli, bir **kodlayÄ±cÄ±** ve bir **kod Ã§Ã¶zÃ¼cÃ¼den** oluÅŸur. AyrÄ±ca uzun kÄ±sa sÃ¼reli bellek modelleri gibi geleneksel modellerden daha hÄ±zlÄ± Ã¶ÄŸrenmesini saÄŸlayan, **dikkat mekanizmasÄ± (attention mechanism)** adÄ± verilen bir mekanizmayÄ± iÃ§erir. Bu mekanizma, bir kelimenin anlamÄ±nÄ± anlamak iÃ§in bir cÃ¼mlenin diÄŸer kÄ±sÄ±mlarÄ±na odaklanmayÄ± saÄŸlar. Aradaki iliÅŸkileri keÅŸfetmek iÃ§in eÅŸ zamanlÄ± olarak matematiksel denklemler yÃ¼rÃ¼tÃ¼r. 

### ðŸ§² Attention MekanizmasÄ±

- Kime ne kadar dikkat verilecek?

- Kelimenin hangi baÄŸlamda ne kadar Ã¶nemli olduÄŸunu belirler. Bu sayede daha isabetli anlam Ã§Ä±karÄ±mÄ± yapÄ±lÄ±r.

![alt text](images/attention.png)

- Bir Ã§eviri sistemi var. Daha Ã¶nce geriye doÄŸru kelimelere dikkat azalÄ±rdÄ±. Yani en Ã§ok dikkati â€œdemunisâ€ en az dikkati â€œlesâ€ alÄ±rdÄ± ama ÅŸimdi hangisine ne kadar dikkat vereceÄŸini **attention** ile Ã¶lÃ§Ã¼lÃ¼r. Bu ÅŸekilde en son kelimeye baÄŸlÄ± olma aÅŸÄ±lÄ±yor daha Ã¶nceki kelimeler de dikkate alÄ±nÄ±r.

### ðŸ§² Encoder-Decoder Mimarisi 

![alt text](images/encoder_decoder.png)

- Chatbotlar iÃ§in soru-cevap, Ã§eviriler iÃ§in tr-en vb. ÅŸeklinde eÄŸitilirler.
* **Encoder:** Girdiyi (prompt) analiz eder, anlamlandÄ±rÄ±r.
* **Decoder:** Girdiye dayalÄ± yeni iÃ§erik Ã¼retir (metin, cevap vb.).

---

## ðŸ—ï¸ PopÃ¼ler LLM Modelleri

| Model  | GeliÅŸtirici | Ã–zellik                                              |
| ------ | ----------- | ---------------------------------------------------- |
| BERT   | Google      | Ã‡ift yÃ¶nlÃ¼ anlayÄ±ÅŸ, dil anlama gÃ¶revlerinde baÅŸarÄ±lÄ± |
| GPT-3  | OpenAI      | Soru-cevap, metin Ã¼retimi                            |
| T5     | Google      | Ã‡eviri ve metin Ã¼retimi                              |
| PaLM 2 | Google      | Ã‡ok dilli destek, yÃ¼ksek doÄŸruluk                    |
| BARD   | Google      | Google arama motoruna entegre, LAMDA tabanlÄ±         |

---

## ðŸ› ï¸ Model EÄŸitimi ve Ä°nce Ayar

### ðŸ”¹ Pretraining (Ã–n EÄŸitim)

Wikipedia, kitaplar, GitHub gibi geniÅŸ veri setleri ile eÄŸitilir.

### ðŸ”¹ Fine-Tuning (Ä°nce Ayar)

Model, Ã¶zel bir gÃ¶reve gÃ¶re yeniden eÄŸitilir. Ã–rneÄŸin, mÃ¼ÅŸteri yorumlarÄ±nÄ± sÄ±nÄ±flandÄ±rmak iÃ§in daha az ve etiketli veri ile.

### ðŸ”¹ Prompt Tuning

KullanÄ±cÄ±dan alÄ±nan komutla modelin yÃ¶nlendirilmesidir. Ã–rnek:

```
Customer review: This product is great!
Customer sentiment: positive
```

### ðŸ”¹ PEFT (Parameter-Efficient Fine-Tuning)

Az parametre ile yÃ¼ksek performans saÄŸlar. Ã–rn: LoRA (Low-Rank Adaptation)

---

## ðŸ“¦ HazÄ±r Model KullanÄ±mÄ±

* Hugging Face gibi platformlardan Ã¶nceden eÄŸitilmiÅŸ modeller indirilebilir.
* Transformers kÃ¼tÃ¼phanesi ile kolay entegrasyon yapÄ±labilir.

---

## ðŸ§ª Retrieval Augmented Generation (RAG)

LLMâ€™lerin dÄ±ÅŸ veri tabanlarÄ±ndan bilgi Ã§ekerek daha doÄŸru cevaplar Ã¼retmesini saÄŸlar.

* GerÃ§ek zamanlÄ± veriler, kullanÄ±cÄ±ya Ã¶zel iÃ§erikler, gÃ¼ncel bilgi saÄŸlanabilir.
* HalÃ¼sinasyonlarÄ±n Ã¶nÃ¼ne geÃ§ilmesini saÄŸlar.

---

## âœ… LLM KullanÄ±m AlanlarÄ±

* Metin Ã¼retimi (ÅŸiir, makale, kod)
* Ã‡eviri (otomatik veya talimatla)
* Sohbet robotlarÄ± (chatbot)
* Duygu analizi
* Bilgi alma ve Ã¶zetleme

---

## âš ï¸ LLMâ€™lerin SÄ±nÄ±rlamalarÄ±

* **HalÃ¼sinasyon:** YanlÄ±ÅŸ ve uydurma bilgi Ã¼retimi
* **Ã–nyargÄ± (Bias):** EÄŸitim verilerinden gelen eÅŸitsizlik
* **GÃ¼venlik:** KiÅŸisel verileri sÄ±zdÄ±rma riski
* **RÄ±za:** Verilerin izinli olup olmadÄ±ÄŸÄ± belirsiz olabilir
* **Ã–lÃ§eklenebilirlik:** DonanÄ±m ve kaynak gereksinimi yÃ¼ksektir

---

## ðŸ“Œ SonuÃ§

LLMâ€™ler; gÃ¼Ã§lÃ¼, esnek ve Ã§ok yÃ¶nlÃ¼ araÃ§lardÄ±r. Ancak dikkatli kullanÄ±lmalarÄ±, etik kurallara uyulmasÄ± ve kaliteli veri ile eÄŸitilmeleri gereklidir.

> "Kalite, nicelikten Ã¶nemlidir."

---

## ðŸ”— FaydalÄ± Kaynaklar

* [Hugging Face](https://huggingface.co/)
* [Google AI Blog](https://ai.googleblog.com/)
* [OpenAI](https://openai.com/)
